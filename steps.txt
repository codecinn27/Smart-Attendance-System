
1) setup virtual environment for python 3.10

py -3.10 -m venv env
.\env\Scripts\Activate

2) upgrade pip

python.exe -m pip install --upgrade pip


3) Install torch from  
* make you have install cuda and cudnn 
    * ref: https://github.com/entbappy/Setup-NVIDIA-GPU-for-Deep-Learning
    * reference installing video
        * https://www.youtube.com/watch?v=krAUwYslS8E
        * https://www.youtube.com/watch?v=nATRPPZ5dGE
* reference from https://pytorch.org/get-started/locally/
pip install torch==2.2.2+cu118 torchvision==0.17.2+cu118 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118


4) test pytorch script in test_torch.py

5) run facenet-torch

https://kean-chan.medium.com/real-time-facial-recognition-with-pytorch-facenet-ca3f6a510816

just run this  (facenet_pytorch use torch 2.2 to 2.3)
pip install facenet_pytorch tqdm
dont pip install torch if you have install torch for gpu 


6) pip install opencv-python

7) run code

version 1: main2.py + saved
version 2: main3.py + saved2 + take_pic.py
version 3: dataset-creator.py -> trainer.py -> recognition.py 
    -> include (dataset , sqlite.db, embeddings.pkl)

project/
â”‚
â”œâ”€â”€ app.py                      # ðŸ”¥ Main FastAPI app
â”œâ”€â”€ requirements.txt            # ðŸ“¦ Package dependencies
â”œâ”€â”€ embeddings.pkl              # ðŸ“‚ Saved face embeddings
â”‚
â”œâ”€â”€ templates/                  # ðŸ§© Jinja2 HTML templates
â”‚   â””â”€â”€ recognize.html
â”‚
â”œâ”€â”€ static/                     # ðŸŽ¨ JS, CSS, images (optional)
â”‚
â”œâ”€â”€ recognition/                # ðŸ§  AI-related logic
â”‚   â””â”€â”€ face_recognizer.py      # Face detection + recognition logic


8) Use sqlite instead of json for easy query and display
-> have 4 table
    -> student , class, enrollment, attendance 

    -> What does the enrollment table do?
        It stores which student is enrolled in which class.
        It typically has:

            enrollment_id (primary key, unique ID for each enrollment record)
            student_id (foreign key linking to the student)
            class_id (foreign key linking to the class)

        You can easily find all students in a specific class by querying enrollments with that classâ€™s ID.
        You can find all classes a specific student is enrolled in by querying enrollments with that studentâ€™s ID.
        Keeps your data normalized, avoids duplication.

-> 1st step: create .db with 4 tables
        run table_creation.py 

-> 2nd step: create dummy data to display in forms 
        run dummy_data.py

9) Get the data from the 4 table and display it at record.html

10) Get data from the form in enroll.html and save it into attendance.db and display it at record.html






## Future to add
1) display student details with image


@app.websocket("/ws/enroll_capture/{student_name}")
async def websocket_enroll_capture(websocket: WebSocket, student_name: str):
    await websocket.accept()

    save_dir = os.path.join("static", "dataset", student_name)
    os.makedirs(save_dir, exist_ok=True)

    count = 0
    try:
        while True:
            data = await websocket.receive_text()
            # data like: "data:image/jpeg;base64,...."
            header, encoded = data.split(",", 1)
            img_data = base64.b64decode(encoded)
            np_arr = np.frombuffer(img_data, np.uint8)
            frame = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)

            # Convert BGR to RGB for mtcnn
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Detect faces using MTCNN
            boxes, probs = mtcnn.detect(frame_rgb)

            if boxes is not None and len(boxes) > 0:
                # Take first face detected
                box = boxes[0]
                x1, y1, x2, y2 = [int(b) for b in box]

                # Safety crop with margin
                h, w = frame.shape[:2]
                pad = 20
                y1 = max(0, y1 - pad)
                y2 = min(h, y2 + pad)
                x1 = max(0, x1 - pad)
                x2 = min(w, x2 + pad)

                face_img = frame[y1:y2, x1:x2]

                if face_img.size == 0 or min(face_img.shape[:2]) < 40:
                    await websocket.send_text("no_face")
                    continue

                # Save face image
                img_path = os.path.join(save_dir, f"{count+1}.jpg")
                cv2.imwrite(img_path, face_img)
                count += 1

                await websocket.send_text(f"captured:{count}")

                if count >= 30:
                    await websocket.send_text("done")
                    break
            else:
                await websocket.send_text("no_face")

    except WebSocketDisconnect:
        print("WebSocket disconnected")